{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook will plot the distribution of V1 cell responses to static gratings. The data is 2P calcium responses in the mouse brain, from the Allen Brain Observatory. \n",
    "\n",
    "# The point is to compare the sparsity of responses to static gratings to that during natural scenes. This is a valid comparison because the static grating stimulus conditions are very similar to the natural scene conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# First load the data for a sample mouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all cre lines:\n",
      "\n",
      "[u'Cux2-CreERT2', u'Emx1-IRES-Cre', u'Nr5a1-Cre', u'Rbp4-Cre', u'Rbp4-Cre_KL100', u'Rorb-IRES2-Cre', u'Scnn1a-Tg3-Cre']\n",
      "4\n",
      "Experiment container info:\n",
      "{'session_type': u'three_session_B', 'age_days': 97.0, 'imaging_depth': 375, 'experiment_container_id': 511510653, 'targeted_structure': u'VISpm', 'cre_line': u'Rbp4-Cre', 'id': 510514770}\n"
     ]
    }
   ],
   "source": [
    "#load data (V1, natural image responses) for a sample mouse\n",
    "import os\n",
    "import platform\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cPickle as cpickle\n",
    "\n",
    "from allensdk.core.brain_observatory_cache import BrainObservatoryCache\n",
    "import allensdk.brain_observatory.stimulus_info as stim_info\n",
    "\n",
    "\n",
    "#sets the drive path of hard-disk containing observatory data, based on current OS\n",
    "if platform.system()=='Windows':\n",
    "    drive_path = 'g:/'\n",
    "else:\n",
    "    drive_path = '/Volumes/Brain2016 3/'\n",
    "\n",
    "manifest_path = os.path.join(drive_path, 'BrainObservatory/manifest.json')\n",
    "boc = BrainObservatoryCache(manifest_file=manifest_path)\n",
    "\n",
    "cre_lines = boc.get_all_cre_lines()\n",
    "print(\"all cre lines:\\n\")\n",
    "print(cre_lines)\n",
    "\n",
    "# find all experiment containers that are from V1 and have natural gratings, and choose 1\n",
    "rbp4_ecs = boc.get_experiment_containers(targeted_structures=['VISpm'],cre_lines=['Rbp4-Cre'])\n",
    "\n",
    "container_id = rbp4_ecs[1]['id']\n",
    "print len(rbp4_ecs)\n",
    "\n",
    "exp = boc.get_ophys_experiments(experiment_container_ids=[container_id], \n",
    "                                stimuli=[stim_info.STATIC_GRATINGS])[0]\n",
    "\n",
    "print('Experiment container info:')\n",
    "print exp\n",
    "\n",
    "# get the dff traces, and the stimulus, and pull out the raw dff traces in case we need these later\n",
    "data_set = boc.get_ophys_experiment_data(exp['id'])\n",
    "specimens=data_set.get_cell_specimen_ids()\n",
    "timestamps, traces = data_set.get_dff_traces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stimulus info\n",
    "# The stimulus consisted of a full field static sinusoidal grating at a single contrast (80%). \n",
    "# The grating was presented at 6 different orientations (separated by 30°), 5 spatial frequencies (0.02,0.04,0.08,0.016,0.32)\n",
    "# and 4 phases (0, 0.25, 0.5, 0.75). \n",
    "# The grating was presented for 0.25 seconds, with no inter- grating gray period.\n",
    "# Each grating condition (orientation, spatial frequency, and phase) was presented ~50 times in a random order.\n",
    "# There were blank sweeps (i.e. mean luminance gray instead of grating) presented roughly once every 25 gratings.\n",
    "# these trials were split across three sessions within the same hour (and within same hour as natural scenes)\n",
    "\n",
    "# pull out the stim info\n",
    "stim_table = data_set.get_stimulus_table('static_gratings') #table with image index and start and end times of each trial\n",
    "num_trials = np.shape(stim_table)[0]\n",
    "\n",
    "# pull out the condition id (there will be 121 unique conditions)\n",
    "num_images = 121; #6 orts x 5 sfs x 4 phases +1 for gray\n",
    "orts = [0,30,60,90,120,150,180]\n",
    "sfs = [0.02,0.04,0.08,0.16,0.32]\n",
    "phases = [0, 0.25, 0.5, 0.75]\n",
    "\n",
    "image_id = np.zeros([num_trials])\n",
    "for i in range(num_trials):\n",
    "    if np.isnan(stim_table.orientation[i]): #if it's a blank sweep\n",
    "        image_id[i] = 120\n",
    "    else:\n",
    "        ort_ind = orts.index(stim_table.orientation[i])\n",
    "        sf_ind = sfs.index(round(stim_table['spatial_frequency'][i],2))\n",
    "        phase_ind = phases.index(stim_table.phase[i])\n",
    "        image_id[i] = ort_ind*(20)+sf_ind*(4)+phase_ind\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the mean trial responses are already pre-calculated in allenSDK, so we'll just use those instead of using the code above\n",
    "# to extract single traces and calculating a trial average. But remember that the traces extracted from the NWB (as above) will \n",
    "# have a global (experiment-wide) dff whereas the traces calculated from the SDK will have a local dff. See sweep-response-test\n",
    "# notebook. This shouldn't  produce huge differences in the sparicity stats, so we'll just use the allenSDK values.\n",
    "\n",
    "\n",
    "# For each trial, the ΔF/F for each cell was calculated using the mean fluorescence of the preceding 1 second\n",
    "# as the baseline Fo.\n",
    "\n",
    "# The mean response to each image presentation was defined as the mean change in dff during the 0.5 second period \n",
    "# following the start of the image presentation compared to the 1 second preceding the image presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load pre-calculated trials and trial statisics from the allenSDK module\n",
    "from allensdk.brain_observatory.static_gratings import StaticGratings\n",
    "sg = StaticGratings(data_set)\n",
    "print(\"done analyzing static gratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cell_loc = 0 #pick a sample cell\n",
    "trial_loc = 0 #pick a sample trial\n",
    "\n",
    "\n",
    "# to get the sweep response\n",
    "trial_responses = sg.sweep_response\n",
    "print 'trial_responses is a dataframe of size' +np.str(np.shape(trial_responses))\n",
    "\n",
    "# to get the sweep response for a particular cell and trial\n",
    "sample_response = trial_responses[str(cell_loc)].iloc[trial_loc] \n",
    "print ('sample response is a vector of size ' +np.str(np.shape(sample_response))+' as it includes 1 sec before/after stim')\n",
    "\n",
    "# to get the mean trial response \n",
    "trial_means = sg.mean_sweep_response\n",
    "print ('trial_means is a dataframe of size ' +np.str(np.shape(trial_means)))\n",
    "\n",
    "# to get the mean sweep response for a particular cell and trial\n",
    "sample_mean = trial_means[str(cell_loc)].iloc[trial_loc] \n",
    "print ('sample_mean is a vector of size ' +np.str(np.shape(sample_mean)))\n",
    "\n",
    "\n",
    "# We can also get the mean triaal responses averaged over all 50 presentation of the same image using ns.responses\n",
    "# The response property of the stimulus-specific analysis objects is 7-D array organized with the following dimensions:\n",
    "# 0: # orientations\n",
    "# 1: # spatial frequencies\n",
    "# 2: # phases\n",
    "# 3: # cells (+1 for running)\n",
    "# 4-6: 0=response mean, 1=response standard error of the mean, 2=number of signficant trials\n",
    "\n",
    "print '\\nyou can get the mean response averaged over all 50 repeats of an image using the ns.response module'\n",
    "print np.shape(sg.response)\n",
    "\n",
    "# to get mean trial response averaged over all 50 presentations of an image\n",
    "def get_mean_over_image(sg,cell_loc):\n",
    "    mean_over_image = []\n",
    "    for i in range(6):# orts\n",
    "        for j in np.arange(1,6,1): #sfs\n",
    "            for k in range(4): #phases\n",
    "                mean_over_image.append(sg.response[i,j,k,cell_loc,0])\n",
    "    return np.asarray(mean_over_image)\n",
    "\n",
    "mean_over_image = get_mean_over_image(sg,cell_loc)\n",
    "print np.shape(mean_over_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the response distribution one cell to all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "\n",
    "mean_cell = mean_over_image\n",
    "\n",
    "######################### plot the mean response of one cell to all unique images (mean over 50 repeats of each image)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs = axs.ravel()\n",
    "\n",
    "axs[0].scatter(np.arange(0,num_images-1,1), mean_cell, color='k')\n",
    "axs[0].set_xlabel('Image index')\n",
    "axs[0].set_ylabel('Mean dff response (50 repeats)')\n",
    "axs[0].set_title('response of cell to all unique images')\n",
    "\n",
    "axs[1].hist(mean_cell)\n",
    "axs[1].set_ylabel('Number of responses')\n",
    "axs[1].set_xlabel('Mean dff response (50 repeats)')\n",
    "axs[1].set_title(' responses of cell to all unique images')\n",
    "plt.tick_params(axis='y', which='both', top='off')\n",
    "\n",
    "#measure the kurtosis of the histogram\n",
    "k1=stats.kurtosis(mean_cell, axis=0, fisher=True, bias=True)\n",
    "print 'kurtosis of distribution of responses, averaged over all unique images is '+ np.str(k1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Now define the sparsity of each cell's respones using three metrics:\n",
    "Both metrics will be calculated based on the average (over 50 repeats) response to each unique image.\n",
    "# 1) kurtosis of distribution of average response over 50 repeats. Higher values       indicate higher sparsity\n",
    "# 2) fraction of images that produce a resonse 1SD above or below mean, relative to fraction (0.34) for normal distribution. Higher values indicate higher sparsity.\n",
    "\n",
    "# 3) fraction of images that produce a resonse > +/-2% dff.\n",
    "\n",
    "# We'll see which metric peforms best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# define the indices \n",
    "def get_indk(data):\n",
    "    return stats.kurtosis(data, axis=0, fisher=True, bias=True)\n",
    "\n",
    "def get_indv(data):\n",
    "    num_images = np.shape(data)[0]\n",
    "    mean_resp_cell = np.mean(data)\n",
    "    std_resp_cell = np.std(data)\n",
    "    threshh = mean_resp_cell+std_resp_cell\n",
    "    threshl = mean_resp_cell-std_resp_cell\n",
    "    high_tail_cells = len(np.flatnonzero((data>threshh)))\n",
    "    low_tail_cells = len(np.flatnonzero((data<threshl)))\n",
    "    sprs_indv = 0.34-(high_tail_cells+low_tail_cells)/float(num_images)\n",
    "    return sprs_indv\n",
    "\n",
    "#fixed threshold of 2%dff response (not a good metric, we won't use)\n",
    "def get_indf(data): \n",
    "    num_images = np.shape(data)[0]\n",
    "    return 1-(len(np.flatnonzero(np.abs(data>2)))/float(num_images))\n",
    "\n",
    "# test these indices on a distribution with known kurtosis (normal distribution has kurtosis of 0)\n",
    "mean_over_image = np.random.normal(0,1,1000) #data form normal distribution\n",
    "\n",
    "sprs_indk = get_indk(mean_over_image)\n",
    "sprs_indv = get_indv(mean_over_image)\n",
    "sprs_indf = get_indf(mean_over_image)\n",
    "                 \n",
    "print 'the sparsity index (k) for a normal distribution should be 0, our code gives: ' +np.str(sprs_indk)\n",
    "print 'the sparsity index (v) for a normal distribution should be 0, our code gives: ' +np.str(sprs_indv)\n",
    "print 'the sparsity index (f) does not make sense for a non-dff data so we will not test here'                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now get the sparsity metric for all cells in this mouse and plot distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_cells = np.shape(sg.response[0,0,0,:,0])[0]-1\n",
    "\n",
    "sprs_indk = []\n",
    "sprs_indf = []\n",
    "sprs_indv = []\n",
    "for i in range(num_cells):\n",
    "    mean_over_image = get_mean_over_image(sg,i)\n",
    "    sprs_indk.append(get_indk(mean_over_image))\n",
    "    sprs_indv.append(get_indv(mean_over_image))\n",
    "    sprs_indf.append(get_indf(mean_over_image))\n",
    "\n",
    "\n",
    "# now plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 5))\n",
    "axs = axs.ravel()\n",
    "\n",
    "axs[0].hist(sprs_indk)\n",
    "axs[0].set_ylabel('Number of neurons')\n",
    "axs[0].set_xlabel('Sparsity Index (kurtosis)')\n",
    "plt.tick_params(axis='y', which='both', top='off')\n",
    "\n",
    "axs[1].hist(sprs_indv)\n",
    "axs[1].set_xlabel('Sparsity Index (variable threshold)')\n",
    "plt.tick_params(axis='y', which='both', top='off')\n",
    "\n",
    "axs[2].hist(sprs_indf)\n",
    "axs[2].set_xlabel('Sparsity Index (fixed threshold)')\n",
    "plt.tick_params(axis='y', which='both', top='off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We'll move forward with the first two metrics.\n",
    "Looks like the kurtosis-based and variable threshold indices produce more or less similar results, but the last index based on fixed threshold doesn't. It also reports that the majority of cells are highly sparse, which is in disagreement with the other two metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the first two metrics against each other to see their correlation\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "plt.scatter(sprs_indk,sprs_indv)\n",
    "plt.xlabel('Sparsity Index (kurtosis)')\n",
    "plt.ylabel('Sparsity Index (variable threshold)')\n",
    "plt.title('Comparison of first two metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now get the mean sparsity metric averaged over each mouse, for all transgenic mice in V1, and plot distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cre_lines = ['Cux2-CreERT2', 'Emx1-IRES-Cre', 'Nr5a1-Cre', 'Rbp4-Cre_KL100', 'Rorb-IRES2-Cre', 'Scnn1a-Tg3-Cre']\n",
    "\n",
    "\n",
    "sparsity_dict_cre = {}\n",
    "for cre_line in cre_lines:\n",
    "    print cre_line\n",
    "    container_list_filt = pd.DataFrame(boc.get_experiment_containers(targeted_structures=['VISp'],cre_lines=[cre_line]))\n",
    "\n",
    "    sparsity_dict_cre_container = {}\n",
    "    #for ind in np.arange(len(container_list_filt)):\n",
    "    for ind in np.arange(16):\n",
    "\n",
    "        container_id = container_list_filt['id'].values[ind]  # This goes through the different mice\n",
    "        \n",
    "        print container_id\n",
    "\n",
    "        exp = boc.get_ophys_experiments(experiment_container_ids=[container_id], \n",
    "                                stimuli=[stim_info.STATIC_GRATINGS])[0]\n",
    "        data_set = boc.get_ophys_experiment_data(exp['id'])\n",
    "        sg = StaticGratings(data_set)\n",
    "        num_cells = np.shape(sg.response[0,0,0,:,0])[0]-1\n",
    "        \n",
    "        sprs_indk = []\n",
    "        sprs_indv = []\n",
    "        for i in range(num_cells):\n",
    "            mean_over_image = get_mean_over_image(sg,i)\n",
    "            sprs_indk.append(get_indk(mean_over_image))\n",
    "            sprs_indv.append(get_indv(mean_over_image))\n",
    "\n",
    "        sparsity_dict_cre_container[container_id] = np.mean(sprs_indv)\n",
    "    sparsity_dict_cre[cre_line] = sparsity_dict_cre_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparsity_dict_cre_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the data\n",
    "results_file='static_gratings_v1_sparsity.pkl'\n",
    "f=open(results_file,\"wb\")\n",
    "cpickle.dump(sparsity_dict_cre, f,protocol=cpickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
